\section{Evaluation}
\label{sec:evaluation}

%recognizing URLs
	%% false positives
	%% false negatives
%preventing attacks
	%% about applications 
	%% parameter attacks
	%% file upload attacks
	%% oob attacks
%performance overhead
%not breaking the application

%To evaluate the success of the defense solution, we use the OWASP Vulnerable Web Applications Directory Project~\cite{vwad}. This directory is a collection of vulnerable web applications, which have deliberately made vulnerable to be used for educational purposes. We search this directory to find the ones which are vulnerable either to SSRF or XXE. We settle on about ten web applications. 

The proposed SSRF defense solution aims at mitigating the impact of SSRF attacks by automatically adjusting and sandboxing URLs sent in requests. The solution relies on the ability of recognizing URLs as a primary step. Therefore, we start this section with the evaluation of our URL recognition module. We then present the overall ability of our solution in protecting against SSRF attacks. This is followed by a discussion on the effects of our solution on the performance of web applications. We wrap up this section with the discussion of methods attackers can leverage to evade our defense and how they can be addressed.

\subsection{Recognizing URLs}
Not all web client libraries parse a given URL in the same way. For example, some of the libraries might require the presence of a scheme name (e.g., "http://") in a given URL, while others might already assume it. This difference makes the accurate recognition of URLs a challenging task. Therefore, we analyze several libraries across various popular server-side languages and frameworks to get insight into how they parse the given URLs, and we discuss possible effects of parsing differences on our defense solution.


%The ability to recognize URLs sent in requests, forms the base for this defense solution. However, due to the differences in URL parsing of libraries, Therefore, we evaluate this ability by showing the results from many popular libraries across several server-side languages and frameworks. 

\subsubsection{Collecting Libraries}
To examine the parsing process, we compile a list of popular web client libraries (see Table~\ref{tbl:libraries}). For each server-side programming language, there are usually multiple standard and third-party web client libraries. Normally, standard libraries offered by languages themselves are used most widely. However, in some languages, a third-party library might have the same popularity due to its ease of use and speed. In such cases, we also add those libraries to our list. %We also examine the relationship between libraries and do not include those which internally use the ones already in the list (e.g. in .NET, WebClient wraps HttpWebRequest).  

\subsubsection{False Negatives}
To recognize URLs sent in HTTP requests, we use the scheme separator pattern (i.e., "://"). If a server-side library accepts URLs without a scheme, such URLs will pass unnoticed. In that case, the defense solution will fail to prevent the attack. However, our analysis of web client libraries suggests that accepting URLs without the scheme part is unusual. In fact, as Table~\ref{tbl:libraries} shows, only the "cURL" library of PHP and built-in http client library of Ruby accept URLs without a scheme name, while the rest of them, require this part to be present in the URL.  

If a web application relies on the "cURL" library of PHP or "http/net" of Ruby to make HTTP requests, an attacker can easily bypass our protection by omitting the scheme part of URLs. However, this issue can be easily fixed by adding a check in the application code to fail URLs which do not start with a scheme name.  
%\subsubsection{False Positives}

\begin{table}[t]
\centering
\begin{tabular}{p{0.13\textwidth} l c|c|c}
\hline
\multirow{2}{*}{Library/Function} & \multirow{2}{*}{Language} & \multicolumn{3}{c}{URL Patterns} \\
\cline{3-5} & & http://a.bc/ & //a.bc/ & a.bc/ \\
\hline
\hline
HttpWebRequest & C\# & \checkmark &  &  \\
HttpClient & C\# & \checkmark &  &  \\
net/http & Go & \checkmark &  &  \\
java.net & Java & \checkmark &  &  \\
http & JS (Node.js) & \checkmark &  &  \\
request & JS (Node.js) & \checkmark &  &  \\
libwww-perl & Perl & \checkmark &  &  \\
file\_get\_contents() & PHP & \checkmark &  &  \\
cURL & PHP & \checkmark & & \checkmark \\
urllib & Python & \checkmark &  &  \\
requests & Python & \checkmark &  &  \\
net/http & Ruby & \checkmark & \checkmark &
\\
\hline
\end{tabular}
\caption{Most libraries require URLs to start with a scheme name.}
\label{tbl:libraries}
\end{table}

\subsubsection{False Positives}
Confusing non-URL elements in a request with URLs and modifying them might have unintended consequences. We assume that the pattern we use (i.e., "://") is almost unique to URLs. To verify this assumption, we do text analysis on a hundred of arbitrary \texttt{README.md} pages from Github as they usually have a good combination of technical and non-technical texts. We observe that the scheme separator pattern exclusively belongs to URLs, therefore, it can hardly be found in non-URL elements of a request. As a result, non-URLs elements will hardly be confused as URLs.

\subsection{Preventing Attacks}
To demonstrate how our proposed defense solution prevents SSRF attacks, we deploy it with web applications from OWASP Vulnerable Web Applications Directory (VWAD)~\cite{vwad}. We evaluate how effectively in-band and out-of-band SSRF attacks are prevented. 

\subsubsection{Compiling List of Applications}
Not all applications listed in the OWASP VWAD are vulnerable to SSRF attacks. Usually, known vulnerabilities are listed per each application. Thus, we choose those applications which have Server-Side Request Forgery (SSRF) vulnerability. We also include applications with XXE (External XML Entity) vulnerability in our list as XXE vulnerability usually allows SSRF attacks.

Not all applications in the compiled list are suitable for our experiments. Some of them are restrictive (i.e., do not allow attacks), and there are few others we cannot install (e.g., the installation interface does not work), and thus, cannot deploy. Applications listed in Table~\ref{tbl:app_list} are those we could deploy successfully with our defense solution. We use ID numbers shown in the table to refer to these applications in the remainder of this section.

These applications become vulnerable to SSRF attacks in different ways. Application \#3 accepts a URL to set the profile photo of a user, while application \#2 retrieves stock data from a user-specified source. Application \#6 receives an image URL from a user to download and show it to the user. The rest of applications process XML inputs and become vulnerable.

For each application, we follow the same procedure to prepare it for experiments. We first start each application on our local machine and provide its address to our extended reverse proxy to route all incoming HTTP traffic to that address. Simultaneously, we start the helper server by running a container created from its image.
%We deploy each application in the compiled list shown in Table~\ref{tbl:app_list} with our prototype solution to conduct experiments. ID numbers shown in the table, will be used to refer to the applications. %Although these applications have been developed with different technologies (i.e. language, framework), our defense solution works with all of them. %The compiled list is shown in Table~\ref{tbl:app_list}. 


\begin{table}
\begin{tabular}{c|l}
\hline
ID & Name\\
\hline
\hline
1 & Vulnerable Java Web Application~\cite{vjwa}\\
%Damn Vulnerable Node Application (DVNA)~\cite{dvna} & Node.js\\
2 & NodeGoat~\cite{nodegoat}\\
3 & OWASP Juice Shop~\cite{juice}\\
4 & Magical Code Injection Rainbow~\cite{mcir}\\
5 & Mutillidae~\cite{mutillidae}\\
6 & Xtreme Vulnerable Web Application~\cite{xvwa}\\
%Vulnerable API~\cite{vuln-api} & Python\\
\hline
\end{tabular}
\caption{Names and ID numbers of sample applications, are listed.}
\label{tbl:app_list}
\end{table}

\subsubsection{Preventing In-band Attacks}
In-band attacks make up a much larger portion of overall SSRF attacks. In fact, our analysis of 61 HackerOne vulnerability reports shows that the attack is in-band in nine out of ten cases. 

Experimental results show that the proposed solution can prevent all in-band SSRF attacks. Only one of them requires minimal developer collaboration. This is because the web client library, Node.js Needle, used by the application, accepts URLs without a scheme name. The developer's minimal assistance of adding a check to fail URLs without a scheme name, lets our defense solution be fully effective. Attacks on the rest of the applications are successfully prevented in an automated way. 

This solution is independent of technologies used on a subject system. In fact, as seen in Table~\ref{tbl:results}, applications used in our experiments use different languages and frameworks. Nevertheless, our defense solution works with all of them. % and therefore can work with applications developed with various languages and frameworks. In fact, as seen in table~\ref{tbl:results}, sample applications.

Attack payloads are negated regardless of where they are placed in the request. Current defense mechanisms usually assume that developers know where an attack payload might reside in a request (i.e., usually some POST or GET parameter). Therefore, they usually fail when an attack payload is delivered in an unexpected part of a request. Whereas our defense solution automatically deals with all possible places, an attack payload might be inserted. In fact, as Table~\ref{tbl:results} shows, attacks on selected applications have their payloads inserted in three different ways: 1) as a parameter value, 2) within a value of a parameter, and 3) within a request body (i.e., outside a parameter). %some of the prevented attacks have their payloads sent not as a parameter value (i.e. within parameter, within body). 

Thanks to the search pattern used by our defense solution, attacks involving URLs with a non-http scheme (e.g., gopher, dict) are also prevented although no example of such cases did exist in our experiments. While some of these schemes can enable more complicated attacks, they are supported by very few web client libraries.
\begin{table}[h]
\begin{tabular}{c|l|l|c|c}
\hline
ID & Technology & Payload Place & Automated & Assisted\\
\hline
\hline
1 & Java & within body & \checkmark &\\
2 & Node.js & parameter & & \checkmark\\
3 & Node.js & parameter & \checkmark &\\
4 & PHP & within parameter & \checkmark &\\
5 & PHP & within parameter & \checkmark\\
6 & PHP & parameter & \checkmark &\\
\hline
\end{tabular}
\caption{While one application needs a minimal developer collaboration, in all other applications all in-band attacks are prevented in an automated manner.}
\label{tbl:results}
\end{table}
%in-band makes up the largest portion, 90\%
%example with get request
%example with post request
%example with file upload
%example with unexpected part (XXE)

\subsubsection{Preventing Out-of-band Attacks}
Our analysis of SSRF vulnerability reports suggests that only one out of ten SSRF attacks are of this type. Our defense solution cannot prevent out-of-band attacks in its current state as in this type of attack the payload is not transferred between an attacker and a victim application. Instead, the application retrieves it from an external location specified by the attacker.

Nevertheless, we believe that our defense approach can also be applied to out-of-band attacks by deploying a system in between the application and the Internet to modify URLs contained in downloaded contents. However, in this scenario, URLs will be much more common, and therefore, various challenges might arise in ensuring that the application works as before. %Provided that this system carefully selects types of contents to intercept and modify, it can be a practical solution.

\subsection{Affecting Application Performance}
The proposed defense solution modifies some incoming requests and uses a helper server to fetch resources for an application, and thus, the functionality and speed of the application are both affected.

\subsubsection{Affecting Functionality}
As the solution modifies an incoming request only when its URI or body contains a URL, the majority of requests pass through the reverse proxy unchanged. As a result, most of the application's functionality remains unaffected. For example, static files (e.g., CSS, JavaScript) are loaded in the same way as before, and thus, nothing changes in the appearance. 

Functionality effects come into play when a URL is sent in a request. For instance, when a user submits a URL to upload her profile photo, this HTTP request (sent after "submit" button is clicked) is modified and the photo is retrieved through a helper server.

To see how the functionality of selected applications are affected when they are deployed with our defense solution, we manually test the pages of all applications against their expected output. 

We observe different behavior only in two pages. One of them does a client-side redirection based on a URL input. We solve this by adding the name of that parameter to the list of excluded parameters. The other refuses to download an image over SSL with the self-signed SSL certificate installed on the helper server. This can be fixed with a legitimate SSL certificate. %In one page, an application receives a URL and does a client-side redirection based on that URL. We exclude this page from the modification (which does not create a hole in defense) to provide a proper navigation in the loaded website. Except this, all pages look and function the same way as they do without the protection.


\subsubsection{Affecting Speed}
Our solution affects the speed of web applications, because it comes with two extra operations: 1) processing requests for search and modification, and 2) fetching resources over the helper server. The former applies to all requests coming to the application, whereas the latter happens only when the application needs to fetch a resource from a user-provided URL.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{figs/times_bar.png}
    \caption{For each application, the average response time of the URL submitting request is recorded both when the protection is enabled and disabled.}
    \label{fig:times_bar}
\end{figure}

To process HTTP requests, our prototype benefits from a module that embeds the Lua Just-In-Time (LuaJIT) interpreter into NGINX reverse proxy. Lua in itself has a good reputation for speed, and LuaJIT makes it even faster~\cite{fast-lua}. In addition, our request processing consists mainly of only two types of string searches (i.e., one in the URI and another in the body).

%When either the body or URI of an incoming request contains a URL and subsequently the web application invokes that URL, the helper server comes into play. 
Fetching resources through the helper server increases the Round-Trip Time (RTT) of the request because the helper server is an intermediate node between a web application server and the Internet. However, since the helper server is deployed in the internal network, only two more network hops (one going and another coming back) are added. In addition, the helper server comes into play only when the web application needs to fetch a resource from the Internet. 

%We manually test pages which involve URL submission and also the ones which do not. We find that there is no noticeable delay in the loading of web pages.
%On each application, we measure the average response time of URL submitting requests to see how much slowness our defense solution causes. For each application, we repeat the request ten times and record the average, both with and without our defense solution enabled. 

To measure the extent of the slowdown, we use a tool called Burp Suite~\cite{burp_website}. "Intruder" part of this tool allows running a request many times automatically and provides the information of "Response completed" time for each request. In fact, this is the amount of time taken for the response to complete after the request is sent~\cite{burp-docs}.

We specifically choose the request which involves URL submission from each application, because URL submitting requests faces more slowdown than others. To be more specific, URL submitting requests are slowed down by the combination of both request processing and fetching over the helper server, whereas other requests are only affected by request processing overhead. Therefore, our choice of URL submitting requests for speed measurements, lets us see the extent of the slowdown in the worst-case scenario. 

Then, we run each (URL submitting) request 100 times with the same parameters, with both the protection enabled and disabled. To minimize the effects of outlying values, we calculate the 5\% trimmed mean (after sorting values, 5\% lowest and 5\% highest values are trimmed and 90\% remains to be used in the calculation). %Figure~\ref{fig:impact_time} shows how the speed of web applications is effected before and after enabling our defense system.
%(i.e. 90\% of the values around the median are used in the calculation).

The results given in Figure~\ref{fig:times_bar}, show that our solution does not significantly reduce the speed of applications. In fact, little increase is observed in the response time in applications \#1, \#2, and \#3. Interestingly, for the rest of the applications (\#4, \#5, \#6) the response time has decreased when our protection is enabled. We speculate that this happens because the helper server is dedicated to downloading contents, whereas the web application server is much busier, and therefore, might be a little slower to download data. %We speculate that it happens because the extension code changes the behavior of the reverse proxy in a way that it speeds up the request processing. 

%Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem

%\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
%\begin{table}[t]
%\begin{tabular}{c|M{2cm}|M{2cm}} 
%\hline 
%ID & Response Time (disabled) & Response Time (enabled)\\ 
%\hline 
%\hline 
%1 & 287 ms & 358 ms\\ 
%2 & 103 ms & 160 ms\\ 
%3 & 68 ms & 64 ms\\ 
%4 & 268 ms & 176 ms\\ 
%5 & 337 ms & 247 ms\\ 
%6 & 2872 ms & 2433 ms\\ 
%\hline 
%\end{tabular} 
%\caption{Average response times both when the protection is enabled and disabled, are listed.} 
%\label{tbl:performance_results} 
%\end{table} 

\subsection{Potential Evasion}
%As with other defense techniques, this technique can also be evaded. Evasion methods vary with how this defense technique is deployed.

An important part of our defense solution is URL recognition that is accomplished by pattern matching. Also, our approach can be implemented in essentially two different ways: 1) on an intermediate node sitting between clients and an application, and 2) in the application code. Therefore, in what follows, we first discuss how our URL recognition system is affected by the broad category of URL obfuscation, and then, we explain how evasion methods can affect our defense system depending on the way it is implemented.

\subsubsection{URL Obfuscation}
URL obfuscation techniques have proven to be effective in deceiving defense systems and users~\cite{url-obfuscation}. Attackers might use URL shortening services to hide malicious URLs, or they might abuse URL redirects on a legitimate website to take users (or possibly systems) to malicious websites. The latter is also known as "Unvalidated Redirects and Forwards" vulnerability~\cite{open-redirect}.

However, these techniques pose no threat to our defense solution since all URLs are adjusted without distinction, and as a result, they are handled by the helper server. Therefore, even if manipulated URLs are obfuscated, they cannot inflict any harm to internal services on the target network.

\subsubsection{Intermediate Deployment}
When our defense solution is deployed on an intermediate system between clients and the web application server, it might be bypassed in multiple ways. We discuss three categories of such bypass methods.

%The first category manipulates the process of URL encoding and decoding. In the context of a HTTP request, URL encoding (also known as percent encoding) is used in two different ways: 1) when a request URI data contains a reserved character (e.g. "/"), it should be percent-encoded~\cite{uri-rfc}, and 2) when a form data is submitted, the default encoding type is "application/x-www-form-urlencoded" and it uses percent encoding to encode reserved characters~\cite{forms-w3c}. Therefore, servers usually decode a URI or form data (with the default encoding type) they receive, to avoid confusion in case they contain reserved characters. 

%The first category manipulates the process of URL encoding and decoding. In the context of a HTTP request, URL encoding is used in two different ways: 1) when a request URI data contains a reserved character (e.g. "/"), it should be URL-encoded~\cite{uri-rfc}, and 2) when a form data is submitted, the default encoding type is "application/x-www-form-urlencoded" and it uses URL encoding to encode reserved characters~\cite{forms-w3c}. Therefore, servers usually decode a URI or form data (with the default encoding type) they receive, to avoid confusion in case they contain reserved characters. 

The first category manipulates the process of URL encoding. URL encoding is needed to avoid confusion in the interpretation of requests by servers. In the context of an HTTP request, URL encoding is used in two different ways: 1) a request URI is usually URL-encoded, because it might contain a reserved character (e.g. "/", "?")~\cite{uri-rfc}, and 2) when a form data is submitted, the default encoding type is "application/x-www-form-urlencoded" and it uses URL encoding to encode reserved characters~\cite{forms-w3c}. Therefore, to correctly interpret requests, servers usually decode a URI and form data (if it is the default type) in HTTP requests they receive. 

Attackers can take advantage of URL encoding to bypass the search pattern (i.e., "://") our defense solution uses. For example, when attackers create their payloads, they can use "http\%3a\%2f/" instead of "http://", as they are treated the same by a server when decoding happens. To avoid this complexity brought by URL encoding, defenders can decode the incoming request before they start searching for URLs in the request. 

However, decoding incoming requests opens a door to double encoding bypass methods. Attackers will encode the payload twice to bypass the defense. In this case, the intermediate defense system decodes only once and fails to notice the pattern, and the server decodes one more time to process the payload. Therefore, in our prototype, we do not decode incoming requests. Instead, we search for all encoded combinations of "://" pattern, even though it is computationally more expensive. %To avoid this bypass, the extension decodes the request URI and also request body if the request header of "Content-Type" has the value of "application/x-www-form-urlencoded". 

The second category of bypass methods involves character encodings (e.g. utf-8, ibm037). Character encoding information is usually sent in the "charset" parameter of "Content-Type" request header to let the server know how bytes should be translated into text~\cite{mdn-charset1, mdn-charset2}. It has been shown that multiple encodings might be supported by servers depending on the technologies used on the server side~\cite{charset-bypass}. Attackers can manipulate this to encode their payloads in multiple ways and to evade pattern matching. Therefore, in our prototype, we check the existence of "charset" parameter in requests and require it to be "utf-8" if it exists. 

The final category is HTTP desynchronization attacks which involves HTTP request smuggling. These attacks usually take advantage of different interpretations of request headers by intermediate and back-end systems. Attackers might use these attacks to bypass our defense solution. They can craft malicious requests which are totally missed by the intermediate system and hit the back-end system~\cite{desync-bypass}. These attacks can be prevented by a cooperation between intermediate and back-end systems aiming to ensure that request headers and their values are interpreted the same way. %in a way that the intermediate system misses the payload, whereas the back-end system processes it~\cite{desync-bypass}. These attacks can be prevented by a cooperation between intermediate and back-end systems, which aims to ensure that request headers and their values are interpreted the same way.

\subsubsection{Code-level Deployment}
Our defense technique can also be deployed as part of an application, at the code level. We are aware of only one bypass method for this fashion of defense implementation. This involves omitting the scheme part of URLs by attackers. If the application accepts URLs without a scheme, the searching mechanism would miss those URLs, and therefore, they pass unmodified and might inflict harm if they are malicious. However, this issue can easily be solved by minimal developer assistance where the developer adds some code to refuse to open a connection to URLs which do not contain a scheme.

As opposed to deployment on an intermediate system, code-level deployment has the advantage of having access to the state of a request as the application code sees it. This, in turn, allows the defense technique to be more accurate and be safe from disguised deliveries. Therefore, even though this type of deployment might not bring the same speed benefits of intermediate deployment, it leaves much less room for evasions.

%As opposed to being deployed as on an intermediate system,  
